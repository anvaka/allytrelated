# allytrelated

A crawler for related youtube channels. Why not? Just playing with the data,
maybe nothing major.

It works by performing BFS on youtube channels starting from [TotalBiscuit channel](https://www.youtube.com/user/TotalHalibut).
This will cover ~3 million channels.

Note: there is a bug in the current implementation which causes a memory leak
in OSX/Unix machines. Start the crawler under [forever](https://github.com/foreverjs/forever)
module. It will crawl enough pages before it fails and then it restarts.

# data format

Once crawling is done the final file is written into `youtube-user.json`, which
will take ~700MB on the hard drive. Each record has the following format:

``` json
{
  "id": "UCy1Ms_5qBTawC-k7PVjHXKQ",
  "title": "TotalBiscuit, The Cynical Brit",
  "related": [
    "UC3kJdy9_bXFg8flaSY3RAcQ",
    "UCSvQyDawUyfXzrSq4dTVraQ",
    "UCvk51_ZhXooIukEKlbw08rA",
    "UCqZ0rqkoUeYlcxlUyqSgpdg",
    "UCCbfB3cQtkEAiKfdRQnfQvw",
    "UC_ufxdQbKBrrMOiZ4LzrUyA",
    "UC90ThxjTNaHaqyPVtfyZ4hw",
    "UCWCw2Sd7RlYJ2yuNVHDWNOA",
    "UCS2OAdHoLt-9T6cG9A2H49Q"
  ],
  "subscribers": "2,104,917",
  "relatedTitle": "Check out our colleagues"
}
```

Note: YouTube has couple `related` sections. By default this crawler will take
only the first one (most of the time the first section is generated by users).

Once crawling is done, you can compact the graph by running:

```
node --max-old-space-size=10000 toBinaryGraph.js
```

This will store graph into `data` folder using [ngraph.tbinary](https://github.com/anvaka/ngraph.tobinary), 
which compcats the data set 6.6x times (from `737MB` down to `112MB`)

# notes

I'm currently experimenting with clustered graph layout, and this repository
consist of crawler + clustering utilities. Ideally it all should be split up
into multiple repositories, but I'm not sure which experiment will survive and
which one will go away. So keeping it here.

# license

MIT
